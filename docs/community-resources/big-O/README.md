# BIG-O

Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. Big O is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmannâ€“Landau notation or asymptotic notation. - [wikipeadia](https://en.wikipedia.org/wiki/Big_O_notation)

we say that an algorithm is O(f(n)) if the
number of simple operations the
computer has to do is eventually less than
a constant times f(n), as n increases

```
f(n) could be linear (f(n) = n)
f(n) could be quadratic (f(n) = n^2)
f(n) could be constant (f(n) = 1)
```

  - O(1)   -> *constant*
  - O(n)   -> *linear*
  - O(n^2) -> *quadratic*

```
Big-O:
  - complexity
  - time complexity
  - algorithmic complexity
  - asymptotic complexity
```

```
Determining Big-O
  - Identify your code
  - Identify N
  - Count the steps in a typical run
  - Keep the most significant part
```


### Resources
  - [Introduction to Big O Notation and Time Complexity (Data Structures & Algorithms #7)](https://www.youtube.com/watch?v=D6xkbGLQesk)
  - [Big O Notation](https://www.youtube.com/watch?v=v4cd1O4zkGw)
  - [You MUST learn this BEFORE you learn algorithms (Big O)](https://www.youtube.com/watch?v=oJ5s2hs_cKk)
  - [Big O: How Code Slows as Data Grows](https://www.youtube.com/watch?v=Ee0HzlnIYWQ)
  
  
